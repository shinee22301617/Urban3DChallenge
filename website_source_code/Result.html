<!DOCTYPE html>
<html style="font-size: 16px;" lang="en"><head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="utf-8">
    <meta name="keywords" content="Experimental results">
    <meta name="description" content="">
    <title>Result</title>
    <link rel="stylesheet" href="nicepage.css" media="screen">
<link rel="stylesheet" href="Result.css" media="screen">
    <script class="u-script" type="text/javascript" src="jquery.js" defer=""></script>
    <script class="u-script" type="text/javascript" src="nicepage.js" defer=""></script>
    <meta name="generator" content="Nicepage 5.2.4, nicepage.com">
    <link id="u-theme-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i|Open+Sans:300,300i,400,400i,500,500i,600,600i,700,700i,800,800i">
    
    
    
    
    
    
    <script type="application/ld+json">{
		"@context": "http://schema.org",
		"@type": "Organization",
		"name": ""
}</script>
    <meta name="theme-color" content="#478ac9">
    <meta property="og:title" content="Result">
    <meta property="og:type" content="website">
  </head>
  <body class="u-body u-xl-mode" data-lang="en"><header class="u-clearfix u-header u-header" id="sec-27cb"><div class="u-clearfix u-sheet u-sheet-1">
        <nav class="u-menu u-menu-one-level u-offcanvas u-menu-1">
          <div class="menu-collapse" style="font-size: 1rem; letter-spacing: 0px;">
            <a class="u-button-style u-custom-left-right-menu-spacing u-custom-padding-bottom u-custom-top-bottom-menu-spacing u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="#">
              <svg class="u-svg-link" viewBox="0 0 24 24"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#menu-hamburger"></use></svg>
              <svg class="u-svg-content" version="1.1" id="menu-hamburger" viewBox="0 0 16 16" x="0px" y="0px" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg"><g><rect y="1" width="16" height="2"></rect><rect y="7" width="16" height="2"></rect><rect y="13" width="16" height="2"></rect>
</g></svg>
            </a>
          </div>
          <div class="u-custom-menu u-nav-container">
            <ul class="u-nav u-unstyled u-nav-1"><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="Intro.html" style="padding: 10px 20px;">Intro</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="RandLA-Net.html" style="padding: 10px 20px;">RandLA-Net</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="BEV-Seg-Net.html" style="padding: 10px 20px;">BEV-Seg-Net</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="Result.html" style="padding: 10px 20px;">Result</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="Team.html" style="padding: 10px 20px;">Team</a>
</li></ul>
          </div>
          <div class="u-custom-menu u-nav-container-collapse">
            <div class="u-black u-container-style u-inner-container-layout u-opacity u-opacity-95 u-sidenav">
              <div class="u-inner-container-layout u-sidenav-overflow">
                <div class="u-menu-close"></div>
                <ul class="u-align-center u-nav u-popupmenu-items u-unstyled u-nav-2"><li class="u-nav-item"><a class="u-button-style u-nav-link" href="Intro.html">Intro</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="RandLA-Net.html">RandLA-Net</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="BEV-Seg-Net.html">BEV-Seg-Net</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="Result.html">Result</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="Team.html">Team</a>
</li></ul>
              </div>
            </div>
            <div class="u-black u-menu-overlay u-opacity u-opacity-70"></div>
          </div>
        </nav>
      </div></header>
    <section class="u-clearfix u-section-1" id="sec-991e">
      <div class="u-clearfix u-sheet u-sheet-1">
        <h1 class="u-text u-text-1">Expe<span style="font-weight: 700;"></span>rimental results
        </h1>
        <p class="u-custom-font u-heading-font u-text u-text-2">The evaluation standard we used is Mean Intersection over Union(mioU), the formula is&nbsp;</p>
        <img class="u-image u-image-contain u-image-default u-image-1" src="images/image10.png" alt="" data-image-width="444" data-image-height="86">
        <p class="u-custom-font u-heading-font u-text u-text-3"> Among them, ùëñ represents the real value, ùëó is the predicted value, and ùëùùëñùëó means that class ùëñ is predicted as class ùëó</p>
        <p class="u-text u-text-4"> mIoU can also be represented as this graph:</p>
        <img class="u-image u-image-contain u-image-default u-image-2" src="images/image11.png" alt="" data-image-width="265" data-image-height="164">
        <img class="u-image u-image-contain u-image-default u-image-3" src="images/image21.png" alt="" data-image-width="478" data-image-height="56">
        <p class="u-text u-text-5"> mIoU is to calculate the overlap ratio of two circles, the formula is:</p>
        <p class="u-text u-text-6">In the experiment of RandLA-Net, the hardware environment we used are:<br> CPU : Intel Core i7-6700<br>RAM : 64GB<br>GPU : GeForce GTX 1080 VRAM 8GB<br>
          <br>The initial setting of RandLA-Net is:<br>Batch Size = 6, Data Size = 30000<br>random sampling<br>
        </p>
        <p class="u-text u-text-7"> The experimental result gained from change of the three variables mentioned above<br>
          <span style="font-weight: 700;">1. Data augmentation</span>
        </p>
        <img class="u-image u-image-contain u-image-default u-image-4" src="images/image41.png" alt="" data-image-width="652" data-image-height="206">
        <p class="u-text u-text-8"> Random and fixed rotation, random translation, scaling, Gaussian noise, and Tensorflow built-in data augment function were performed on the input data respectively, and all were tested in the same environment. The results are shown in the table, without any&nbsp; The mIoU of baseline which is without any data augmentation is 48.223. Although there are obvious differences in the performance of the models with data augmentation, all the performances are worse than the baseline, and the average mIoU is about 42, which is far from the expected results. Since the experimental results showed that data augmentation doesn't help, we later abandoned the modification for this part.</p>
      </div>
    </section>
    <section class="u-clearfix u-section-2" id="sec-6436">
      <div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-text u-text-default u-text-1">2. <span style="font-weight: 700;">Change the ratio between batchsize and data size</span>
          <br>
          <br>
          <span style="font-weight: 400;">The change of the ratio between batchsize and data size on GPU GTX Titan X will be showed in the following pages.</span>
          <br>
          <br>
          <br>3.<span style="font-weight: 700;"> Sample method</span>
          <br>
          <br>
          <span style="font-weight: 400;"></span><b>
            <span style="font-weight: 400;">1. The 3000 pieces of data comes from the same small file.<br>&nbsp; 
            </span></b>
          <span style="font-weight: 400;">&nbsp; &nbsp; &nbsp; &nbsp; <b>&nbsp;<span style="font-weight: 400;">Easier in sampling.</span>
              <br></b>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
          </span><b>
            <span style="font-weight: 400;">Low point cloud diversity</span>
            <br>
            <span style="font-weight: 400;">2. The 3000 pieces of data comes from 33 file evenly. (2000/33 = 90)<br>
            </span></b>
          <span style="font-weight: 400;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span><b>
            <span style="font-weight: 400;">The data gets more average than the previous method.</span>
            <br></b>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<b>
            <span style="font-weight: 400;">The data gets more average than the previous method.<br>3. The 3000 pieces of data comes from 33 file based on the original file size
            </span>
            <br></b>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<b>
            <span style="font-weight: 400;">The data comes average.</span>
            <br></b>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<b>
            <span style="font-weight: 400;">Needs more time to calculate the FPS sampling</span></b>
          <br>
        </p>
        <img class="u-image u-image-contain u-image-default u-image-1" src="images/image101.png" alt="" data-image-width="643" data-image-height="120">
        <p class="u-text u-text-default u-text-2">
          <br> During the training of RandLA-net, we found that the results were not satisfactory in the process of using GTX1080 with 8GB VRAM , so we borrowed another GPU from the laboratory for training, and the configuration of this computer is:<br>CPU Intel Core i7-6700<br>RAM 64GB<br>GPU GeForce GTX Titan X VRAM 12GB<br>
          <br>With the upgrade of size of VRAM, we can increase the batchsize from 180,000 to 240,000, and we increase the datasize to 6*40,000 (compared with 8G VRAM to increase the Data size), and also tried to use the initial setting of 4*65536 proposed by the author of RandLA-Net, while using the same Anaconda virtual environment for training, the main point of change is the ratio between the batchsize and the data size, the results are as follows:<br>
        </p>
        <img class="u-image u-image-contain u-image-default u-image-2" src="images/image12.png" alt="" data-image-width="611" data-image-height="151">
      </div>
    </section>
    <section class="u-clearfix u-section-3" id="sec-e253">
      <div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-text u-text-default u-text-1"> It can be seen that after increasing the size of VRAM, the performance of the overall model has been greatly improved. It shows that RandLA-Net is very sensitive to the size of the input data. After testing different ratios, we found that the ratio of 24*10000 will probably be a sweet spot of input size in this model, and ‚Äãcontinuously&nbsp;increasing the batch size or data size will not get better performance.<br>
          <br>
          <br>
          <br>For the RandLA-Net part, we also made statistics on the training speed. Although running on the CPU can use 64GB of memory, it takes a lot of time, and if it is running on the GTX1080 It takes about 20-25 minutes to run an Epoch. It can be seen that the current machine learning part still needs GPU operations to be faster. The data we tested are as follows:
        </p>
        <img class="u-image u-image-contain u-image-default u-image-1" src="images/image13.png" alt="" data-image-width="805" data-image-height="204">
      </div>
    </section>
    <section class="u-clearfix u-section-4" id="sec-ad61">
      <div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-text u-text-default u-text-1"> In the experiment of BEV-seg, the hardware environment we used are:<br> CPU : Intel Core i7-6700<br>RAM : 64GB<br>GPU : ‚ÄãGeForce GTX TITAN X VRAM 12GBÔºâand GeForce GTX 1080&nbsp;(VRAM 8GB)<br>
          <br>The initial setting of BEV-Seg-Net is:<br>The model input bird's eye view size is 500x500<br>The original input image only has three channels of R, G, and B<br>
          <br> We divide the system implementation into two variables for comparison and performance evaluation:<br>
          <br>
          <span style="font-weight: 700;">1. Change the input image size:</span>
          <br>Due to the hardware limitation of the GPU, if we operate with the initial image size, the batch size can only be set to 1. The effect of the experiment is too poor, so we rescale to 100x100 and use RGB as our baseline, its mIoU is 16.32.<br>
          <br>
          <br>
        </p>
        <img class="u-image u-image-contain u-image-default u-image-1" src="images/image15.png" alt="" data-image-width="1262" data-image-height="226">
        <p class="u-text u-text-default u-text-2">With the size of 100x100 of input, the batchsize can be increased to 32 in the GeForce GTX 1080 (VRAM 8GB)&nbsp;<br>
          <br>
          <br>
          <span style="font-weight: 700;">2. Add the new channel, Altitude</span>
          <br>Since the result of only using RGB is not good, we think that in the case of a bird's-eye view, what is most needed is the feature of height of the corresponding point of each picture, so we use the height as its fourth channel, and get the following results:<br>
          <br>
        </p>
        <img class="u-image u-image-contain u-image-default u-image-2" src="images/image16.png" alt="" data-image-width="609" data-image-height="215">
        <p class="u-text u-text-default u-text-3">As the figure above shows, ‚Äãthe image size of 100x100 is trained on GeForce GTX 1080 VRAM 8GB), and each Batch Size is 32; the image size of 200x200 is trained on GeForce GTX TITAN X VRAM 12GB), and its Batch Size is 16; ‚Äãthe image size of 300x300 is also trained on GeForce GTX TITAN X VRAM 12GB), and its Batch Size can only be opened to 8.<br>
          <br>The first thing that we can find from the above data is that with same image size and epoch number, there is a huge difference between with altitude channel and without. The mIoU of w/o altitude&nbsp; is only 16.32; but the mIoU of with altitude can reach 57.61. You can see that when using bird's-eye view projection for semantic segmentation, adding altitude channel can effectively improve the performance of model.<br>
          <br>When looking at the data that also uses four channels, we can found that the size of the batchsize has a great influence. However, the larger picture (300x300) performs better on the small category (Rail). We speculate that it is because of the picture rescaling, small classes of objects such as Rails and Bikes may be too blurry to be recognized.<br>We also count the time of each experiment, and the time spent in each Epoch is mainly related to the size of the input image.<br>
          <br>
          <br>
          <br>
        </p>
        <img class="u-image u-image-contain u-image-default u-image-3" src="images/image17.png" alt="" data-image-width="612" data-image-height="117">
      </div>
    </section>
    <section class="u-clearfix u-section-5" id="sec-1f77">
      <div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-text u-text-default u-text-1"> Our experimental results:</p>
        <img class="u-image u-image-contain u-image-default u-image-1" src="images/image18.png" alt="" data-image-width="751" data-image-height="620">
        <p class="u-text u-text-default u-text-2">Baseline of each models :</p>
        <img class="u-image u-image-contain u-image-default u-image-2" src="images/image19.png" alt="" data-image-width="755" data-image-height="321">
      </div>
    </section>
    
    
    <footer class="u-align-center u-clearfix u-footer u-grey-80 u-footer" id="sec-195b"><div class="u-clearfix u-sheet u-sheet-1"></div></footer>
    <section class="u-backlink u-clearfix u-grey-80">
      <a class="u-link" href="https://nicepage.com/website-templates" target="_blank">
        <span>Website Templates</span>
      </a>
      <p class="u-text">
        <span>created with</span>
      </p>
      <a class="u-link" href="" target="_blank">
        <span>Website Builder Software</span>
      </a>. 
    </section>
  
</body></html>